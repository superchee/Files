{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "# functions needed for pr_auc_score()\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# functions needed for imbalanced_cross_validation_score()\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# sampler objects\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, NeighbourhoodCleaningRule, AllKNN, InstanceHardnessThreshold\n",
    "\n",
    "# Classification models to compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.svm import SVC # SVM\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_auc_score(clf, x, y):\n",
    "    '''\n",
    "        This function computes area under the precision-recall curve. \n",
    "    '''\n",
    "      \n",
    "    precisions, recalls,_ = precision_recall_curve(y, clf.predict_proba(x)[:,1], pos_label=1)\n",
    "    \n",
    "    return auc(recalls, precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Confusion_matrix(clf, x, y):\n",
    "    cm = metrics.confusion_matrix(y, clf.predict(x))\n",
    "    mcc = matthews_corrcoef(y, clf.predict(x))\n",
    "    \n",
    "    return (cm[0][0], cm[0][1], cm[1][0], cm[1][1], mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalanced_cross_validation_score(clf, x, y, cv, scoring, sampler):\n",
    "    \n",
    "    cv_score = 0.\n",
    "    train_score = 0.\n",
    "    test_score = 0.\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    MCC = 0\n",
    "    \n",
    "    # stratified k-fold creates folds with the same ratio of positive \n",
    "    # and negative samples as the entire dataset.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=0, shuffle=False)\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(x,y):\n",
    "        \n",
    "        xfold_train_sampled, yfold_train_sampled = sampler.fit_sample(x[train_idx],y[train_idx])\n",
    "        clf.fit(xfold_train_sampled, yfold_train_sampled)\n",
    "        \n",
    "        TN_train, FP_train, FN_train, TP_train, mcc_train = scoring(clf, xfold_train_sampled, yfold_train_sampled)\n",
    "        TN_test, FP_test, FN_test, TP_test, mcc_test  = scoring(clf, x[test_idx], y[test_idx])\n",
    "        # tn, fp, fn, tp\n",
    "        print(\"Train TP: {0} Train FP: {1} Train FN: {2} Train TN: {3}; Test TP: {4} Test FP: {5} Test FN: {6} Test TN: {7}\".format(TP_train, FP_train, FN_train, TN_train, TP_test, FP_test, FN_test, TN_test))\n",
    "        print(\"MCC train: {0} and MCC test: {1}\".format(mcc_train, mcc_test))\n",
    "        \n",
    "        TP += TP_test\n",
    "        FP += FP_test\n",
    "        FN += FN_test\n",
    "        TN += TN_test\n",
    "        MCC += mcc_test\n",
    "\n",
    "    ave_tp = TP/cv\n",
    "    ave_fp = FP/cv\n",
    "    ave_fn = FN/cv\n",
    "    ave_tn = TN/cv\n",
    "    ave_mcc = MCC/cv\n",
    "    \n",
    "    sensitivity = ave_tp/(ave_tp + ave_fn)\n",
    "    specificity = ave_tn/(ave_fp + ave_tn)\n",
    "    \n",
    "    g_mean = math.sqrt(sensitivity * specificity)\n",
    "    \n",
    "    values = [sensitivity, specificity, g_mean, ave_mcc]\n",
    "    \n",
    "        \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o = pd.read_csv('financial_data.csv')\n",
    "y_train_o = pd.read_csv('revealed_businesses.csv')\n",
    "\n",
    "x_test_o = pd.read_csv(\"testing_data.csv\")\n",
    "\n",
    "x_train_o.replace('?', np.nan, inplace=True)\n",
    "x_train_o = x_train_o.astype('float64')\n",
    "\n",
    "\n",
    "x_test_o.replace('?', np.nan, inplace=True)\n",
    "x_test_o = x_test_o.astype('float64')\n",
    "\n",
    "data_all = x_train_o.merge(y_train_o, on='Var1', how = 'left')\n",
    "\n",
    "data_nolabel = data_all[data_all.Var66.isnull()]\n",
    "data_label = data_all[data_all.Var66.notnull()]\n",
    "\n",
    "data_nolabel_v = data_nolabel.drop(columns=['Var1', 'Var66'])\n",
    "data_nolabel_id = data_nolabel['Var1']\n",
    "\n",
    "data_label_v = data_label.drop(columns=['Var1', 'Var66'])\n",
    "data_label_id = data_label['Var1']\n",
    "\n",
    "x_test_v = x_test_o.drop(columns=['Var1'])\n",
    "\n",
    "# data_all_v = data_all.drop(columns=['Var1', 'Var66'])\n",
    "# data_all_v_mean = data_all_v.mean()\n",
    "# data_all_v_f = data_all_v.fillna(data_all_v_mean)\n",
    "# minmax_scaler = preprocessing.MinMaxScaler().fit(data_all_v)\n",
    "\n",
    "# data_nolabel_v_f = data_nolabel_v.fillna(data_all_v_mean)\n",
    "# data_label_v_f = data_label_v.fillna(data_all_v_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selected = ['Var28', 'Var22', 'Var25', 'Var7', 'Var27', 'Var17', 'Var35', 'Var30',\n",
    "#        'Var6', 'Var63']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = pd.concat([data_label_v, x_test_v], axis=0)\n",
    "\n",
    "# x_all_s = x_all[feature_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_f = x_all.fillna(x_all.mean()).values\n",
    "x_all_f_scale = preprocessing.Normalizer().fit_transform(x_all_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_all_f_scale[:4879,:]\n",
    "y = data_label['Var66'].values\n",
    "\n",
    "x_test_scale = x_all_f_scale[4879:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbs = DBSCAN(eps=0.01, min_samples=8, metric='cosine', algorithm='auto')\n",
    "dbs.fit(x)\n",
    "pred_y = dbs.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    3753\n",
       "-1     942\n",
       " 1      59\n",
       " 2      43\n",
       " 3      30\n",
       " 5      18\n",
       " 4      11\n",
       " 7       8\n",
       " 6       8\n",
       " 8       7\n",
       "Name: Var66, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(pred_y, columns=[\"Var66\"])\n",
    "df_pred['Var66'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster_train = pd.concat([pd.DataFrame(x), df_pred], axis=1)\n",
    "x_cluster_train.columns = data_label.drop(columns=['Var1']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = []\n",
    "for i in list(range(1,9)):\n",
    "    df_temp = x_cluster_train[x_cluster_train.Var66 == i]\n",
    "    mean_point = list(df_temp.drop(columns=['Var66']).mean())\n",
    "    centroid.append(mean_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucliden(X, c):\n",
    "    c = c.reshape(1, X.shape[1]) # 1 x p\n",
    "    distances = np.sqrt(np.sum((X - c) ** 2, axis=1))\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cluster_0 = x_cluster_train[x_cluster_train.Var66 == 0].drop(columns=['Var66']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cluster_1 = x_cluster_train[x_cluster_train.Var66 == -1].drop(columns=['Var66']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(centroid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for test_x in centroid:\n",
    "    distances = eucliden(main_cluster, np.array(test_x))\n",
    "    indexs = np.argsort(distances)[0:3]\n",
    "    k_dist = distances[indexs].sum()\n",
    "    result.append(k_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5980734070060256,\n",
       " 1.1068394016098777,\n",
       " 3.3078698967537314,\n",
       " 0.6817611043957903,\n",
       " 2.29201161722656,\n",
       " 4.029493241451158,\n",
       " 0.8190006607979017,\n",
       " 0.5710380439597493]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = []\n",
    "for test_x in centroid:\n",
    "    distances = eucliden(main_cluster_1, np.array(test_x))\n",
    "    indexs = np.argsort(distances)[0:3]\n",
    "    k_dist = distances[indexs].sum()\n",
    "    result_1.append(k_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6009796117175947,\n",
       " 0.81217752096967,\n",
       " 0.6541795033472869,\n",
       " 0.5890299791387115,\n",
       " 0.5714436487795931,\n",
       " 0.8675219228224091,\n",
       " 0.5264035082575742,\n",
       " 0.571097395496261]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster_train[x_cluster_train.Var66 == 3]['Var66'] = 100.0\n",
    "x_cluster_train[x_cluster_train.Var66 == 6]['Var66'] = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_1 = x_cluster_train[x_cluster_train.Var66 == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster_train[x_cluster_train.Var66 == 3]['Var66'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>...</th>\n",
       "      <th>Var57</th>\n",
       "      <th>Var58</th>\n",
       "      <th>Var59</th>\n",
       "      <th>Var60</th>\n",
       "      <th>Var61</th>\n",
       "      <th>Var62</th>\n",
       "      <th>Var63</th>\n",
       "      <th>Var64</th>\n",
       "      <th>Var65</th>\n",
       "      <th>Var66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Var2, Var3, Var4, Var5, Var6, Var7, Var8, Var9, Var10, Var11, Var12, Var13, Var14, Var15, Var16, Var17, Var18, Var19, Var20, Var21, Var22, Var23, Var24, Var25, Var26, Var27, Var28, Var29, Var30, Var31, Var32, Var33, Var34, Var35, Var36, Var37, Var38, Var39, Var40, Var41, Var42, Var43, Var44, Var45, Var46, Var47, Var48, Var49, Var50, Var51, Var52, Var53, Var54, Var55, Var56, Var57, Var58, Var59, Var60, Var61, Var62, Var63, Var64, Var65, Var66]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 65 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cluster_train[x_cluster_train.Var66 == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = SMOTE(kind='svm', sampling_strategy=0.7,random_state=42).fit_sample(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=92,learning_rate=0.2,scale_pos_weight=0.5, max_depth=5, subsample=0.7, n_estimators=500, gamma=0, colsample_bytree=0.9)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred_dt = clf.predict(x_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1423\n",
       "1.0      77\n",
       "Name: Is_Bankrupted, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_business_id = x_test_o['Var1']\n",
    "df_y = pd.DataFrame(y_pred_dt, columns=[\"Is_Bankrupted\"])\n",
    "upload = pd.concat([x_test_business_id, df_y], axis=1)\n",
    "df_y[\"Is_Bankrupted\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = upload.astype('int32')\n",
    "upload.columns=['Business_ID', 'Is_Bankrupted']\n",
    "upload.to_csv('4_15_73.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
